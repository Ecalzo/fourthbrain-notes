# 2.4 Practical Gradient Descent II

## Making sure gradient descent is working correctly
Typically we will plot the output of the cost function by number of iterations to ensure that the loss is reducing as we iterate.

If gradient descent is working properly, then J(Î¸) should decrease after every iteration.

This graph will also help you understand if gradient descent has converged.

It turns out that it can be very difficult to predict how many iterations gradient descent will need in order to converge.

## Automatic Convergence Tests
It is also possible to come up with automatic convergence tests.

Declares convergence if your cost function decreases by less than 10<sup>-3</sup> in one iteration. Andrew tends to plot in order to decide on how many iterations, rather than trying to pick a static delta value for the convergence check.

## Learning Rate
You may want to use a smaller learning rate if you find that your loss is increasing at some points. Also double check for bugs!

For sufficiently small alpha, the cost funciton should decrease on every iteration. But if it is too small, Gradient Descent may be slow to converge (more iterations).

When choosing alpha, try out a few values `[.001, .003, .01, .03, .1, .3, ...]`

Try to pick the largest reasonable value to reduce the number of iterations.
