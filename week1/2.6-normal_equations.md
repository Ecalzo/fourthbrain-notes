# 2.6 Normal Equations
Normal equations are another way of finding a local minimum.

Typically, in calculus, we take a derivative of a quadratic equation and set it equal to 0 in order to solve for a local optimum.

To apply this to our vector of thetas, we find that we need to set each derivative of each value of theta equal to 0 in order to solve for the local optima of each.

## GD vs. Normal Equations
Gradient Descent
* Need to choose alpha
* Need many iterations
* Works well even when n (features) is large

Normal Equation
* No need to choose alpha
* Don't need to iterate
* Need to compute (X<sup>T</sup>X)<sup>-1</sup>
* Slow if n is very large b/c these are n x n matrices - computing the inverse is roughly O(n<sup>3</sup>)
