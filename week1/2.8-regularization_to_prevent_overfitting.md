# 2.8 Regularization to Prevent Overfitting
## What is overfitting?
* Underfitting, or "high bias" - means that a model does not fit the data very well. Bias is implying that the model is somewhat opinionated despite data telling it otherwise.
* Overfitting, or "high variance" - means that our model nearly passes through every single training data point. High variance implies that this model could fit almost any data set, there are too many hypothesis to account for. **The model will then fail to generalize to new examples** that it has not yet seen.

The same issues can apply to logistic regression as well.


## Addressing Overfitting
We will not always have two dimensional data that we can graph nicely to select an appropriate polynomial degree.

More often, we will have two many features to visually inspect any instance of overfitting.

1. Reduce number of features
    * Manually select which features to keep
    * Model selection algorithm (later in 
    course)
    * Throwing away some of the features is not an ideal solution - that data is useful!
2. Regulariztion
    * Keep all of the features, but reduce magnitude/values of parameters Î¸<sub>j</sub>
    * Works well when we have a lot of features that each contribute to predicting `y`
