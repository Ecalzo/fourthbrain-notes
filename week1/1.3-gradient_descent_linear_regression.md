# 1.3 Gradient Descent for Linear Regression
Now we will apply gradient descent to minimize our squared error cost function.

The key term we need is the partial derivative of our cost function, J. This requires multivariate calculus.

We will then use the gradient in our gradient descent algorithm.

## Gotchas
Depending on where you initialize gradient descent, you can end up at a different local optima. The good news is that linear functions are always "bow-shaped" or convex. Performing gradient descent on a linear regression will always yeild a global minimum, since there is only one global minimum for the convex shape of a multivariate linear regression.

## "Batch" Gradient Descent
Each step of gradient descent uses _all_ the training examples.
* We will eventually see other forms of batching
