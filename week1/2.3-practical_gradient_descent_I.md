# Practical Gradient Descent I
## Feature Scaling:
* Make sure that features are on a similar scale
* This is because Gradient Descent will take much longer to find a global minimum if θ<sub>0</sub> and θ<sub>1</sub> are significantly different
    * this can be visualized by plotting θ<sub>0</sub> and θ<sub>1</sub> as x and y, respectively.
* Since our cost function is also a function of `x`, by scaling our features, in the vector `x`, we are also improving our ability for perform gradient descent and minimize our cost function
* In his example, Andrew normalizes by dividing each feature by the largest number in the feature set.
Ideally, we will like to have every feature -1 < x<sub>i</sub> <= 1

## Mean Normalization
Replace x<sub>i</sub> with x<sub>i</sub> - μ<sub>i</sub> to make features have approximately zero mean.
* Do not apply to x<sub>0</sub> = 1

ex: x<sub>1</sub> = `size - mean(size) / max(size) - min(size)`

