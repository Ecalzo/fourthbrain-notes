# 1.2 Gradient Descent Intuition
We can picture gradient descent on a function with just a single theta input. The derivative of the function will return the slope, the slope will carry with it the sign (pos, neg). In our update step, this informs in which direction theta should move.

## What if theta is already at a local minimum? What will Gradient descent do?
If we are at a local optimum already, the slope of the function will be 0. THis means that the gradient is zero and our theta parameter will cease to update.

## alpha
* Alpha is too small
    * Gradient descent can take a very long time to reach a local minimum
* Alpha is too large
    * Gradient descent can take huge steps and consistently overshoot the local minimum
    * This can fail to converge and even diverge

Gradient descent can converge to a local minimum, even with the learning rate, alpha, fixed.
* This is because, generally, the magnitude of the slope will reduce as we approach a local minimum.

