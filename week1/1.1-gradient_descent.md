# 1.1 Gradient Descent
A general algorithm for minimizing a function. We will be mainly applying it to the cost function we would like to minimize.


Gradient descent is like standing on the highest hill in a park and asking:
> "if I look around and take a baby step in any direction, which direction will most quickly get me to the lowest local minimum?"

It is important to update all values of theta simultaenously.

Generally, the gradient descent algorithm updates a feature vector, in this case θ, as a function of the gradient of a function applied to the feature vector, θ, and a learning rate parameter, α.
